{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üõ°Ô∏è Malware Classification Using Graph Neural Networks on Control Flow Graphs\n",
    "\n",
    "This notebook implements a complete pipeline for binary malware classification using:\n",
    "- **Control Flow Graph (CFG) extraction** with angr\n",
    "- **Graph Neural Networks (GNN)** for classification\n",
    "- **PyTorch Geometric** for graph deep learning\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Data Upload & Preparation](#data)\n",
    "3. [CFG Extraction](#extraction)\n",
    "4. [Feature Engineering](#features)\n",
    "5. [Model Definition](#model)\n",
    "6. [Training](#training)\n",
    "7. [Evaluation & Visualization](#evaluation)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Runtime Configuration\n",
    "\n",
    "**Important:** Enable GPU for faster training!\n",
    "- Go to: **Runtime ‚Üí Change runtime type ‚Üí GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "Install all required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install PyTorch and PyTorch Geometric\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install torch-geometric\n",
    "!pip install pyg-lib torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install angr networkx scikit-learn pandas matplotlib seaborn pyyaml tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import angr\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from google.colab import files\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_cell"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'model_type': 'gcn',  # Options: gcn, gcn_deep, gat, graphsage\n",
    "    'num_features': 10,\n",
    "    'hidden_channels': 64,\n",
    "    'num_classes': 2,\n",
    "    'dropout': 0.5,\n",
    "    'pooling': 'mean',\n",
    "    'epochs': 100,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.0005,\n",
    "    'early_stopping_patience': 15,\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    'use_class_weights': True\n",
    "}\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 2. Data Upload & Preparation\n",
    "\n",
    "### Option A: Upload Binary Files\n",
    "\n",
    "Upload your executable files (benign and malware samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dirs"
   },
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "!mkdir -p data/raw/benign data/raw/malware data/processed\n",
    "\n",
    "print(\"üìÅ Directory structure created:\")\n",
    "print(\"   - data/raw/benign/    (place benign executables here)\")\n",
    "print(\"   - data/raw/malware/   (place malware executables here)\")\n",
    "print(\"   - data/processed/     (processed CFG files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_files"
   },
   "outputs": [],
   "source": [
    "# Upload binary files\n",
    "print(\"üì§ Upload your binary files\")\n",
    "print(\"   1. First, upload BENIGN executables\")\n",
    "print(\"   2. Then, upload MALWARE executables\")\n",
    "print(\"\\n‚ö†Ô∏è  Make sure to organize them correctly!\\n\")\n",
    "\n",
    "# Uncomment the section you want to upload\n",
    "\n",
    "# Upload benign files\n",
    "# print(\"Uploading BENIGN files...\")\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     !mv \"{filename}\" data/raw/benign/\n",
    "# print(f\"‚úÖ Uploaded {len(uploaded)} benign files\\n\")\n",
    "\n",
    "# Upload malware files\n",
    "# print(\"Uploading MALWARE files...\")\n",
    "# uploaded = files.upload()\n",
    "# for filename in uploaded.keys():\n",
    "#     !mv \"{filename}\" data/raw/malware/\n",
    "# print(f\"‚úÖ Uploaded {len(uploaded)} malware files\")\n",
    "\n",
    "print(\"\\nüí° TIP: You can also mount Google Drive and use files from there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "option_b"
   },
   "source": [
    "### Option B: Mount Google Drive\n",
    "\n",
    "If you have files in Google Drive, mount it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Copy files from Drive\n",
    "# !cp /content/drive/MyDrive/your_benign_folder/* data/raw/benign/\n",
    "# !cp /content/drive/MyDrive/your_malware_folder/* data/raw/malware/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_files"
   },
   "outputs": [],
   "source": [
    "# Check uploaded files\n",
    "benign_files = !ls data/raw/benign/ 2>/dev/null | wc -l\n",
    "malware_files = !ls data/raw/malware/ 2>/dev/null | wc -l\n",
    "\n",
    "benign_count = int(benign_files[0]) if benign_files else 0\n",
    "malware_count = int(malware_files[0]) if malware_files else 0\n",
    "\n",
    "print(f\"üìä Dataset Summary:\")\n",
    "print(f\"   Benign samples:  {benign_count}\")\n",
    "print(f\"   Malware samples: {malware_count}\")\n",
    "print(f\"   Total:           {benign_count + malware_count}\")\n",
    "\n",
    "if benign_count == 0 and malware_count == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No files found! Please upload binary files first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "extraction"
   },
   "source": [
    "## 3. CFG Extraction\n",
    "\n",
    "Extract Control Flow Graphs from binary executables using angr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extraction_functions"
   },
   "outputs": [],
   "source": [
    "def get_file_hash(file_path):\n",
    "    \"\"\"Calculate SHA256 hash of a file\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "\n",
    "def strip_none_attributes(G):\n",
    "    \"\"\"Remove None attributes from graph (required for GraphML export)\"\"\"\n",
    "    for node, attrs in list(G.nodes(data=True)):\n",
    "        for k, v in list(attrs.items()):\n",
    "            if v is None:\n",
    "                del attrs[k]\n",
    "    \n",
    "    for u, v, attrs in list(G.edges(data=True)):\n",
    "        for k, val in list(attrs.items()):\n",
    "            if val is None:\n",
    "                del attrs[k]\n",
    "\n",
    "\n",
    "def extract_cfg_from_binary(binary_path, label):\n",
    "    \"\"\"\n",
    "    Extract CFG from a single binary\n",
    "    \n",
    "    Args:\n",
    "        binary_path: Path to the binary file\n",
    "        label: 0 for benign, 1 for malware\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (networkx graph, metadata dict)\n",
    "    \"\"\"\n",
    "    file_hash = get_file_hash(binary_path)\n",
    "    \n",
    "    metadata = {\n",
    "        'filename': os.path.basename(binary_path),\n",
    "        'file_hash': file_hash,\n",
    "        'label': label,\n",
    "        'status': 'failed',\n",
    "        'num_nodes': 0,\n",
    "        'num_edges': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load binary with angr\n",
    "        proj = angr.Project(\n",
    "            binary_path,\n",
    "            load_options={'auto_load_libs': False}\n",
    "        )\n",
    "        \n",
    "        # Generate CFG\n",
    "        cfg = proj.analyses.CFGFast(normalize=True)\n",
    "        G = cfg.graph\n",
    "        \n",
    "        # Strip None attributes\n",
    "        strip_none_attributes(G)\n",
    "        \n",
    "        metadata['status'] = 'success'\n",
    "        metadata['num_nodes'] = G.number_of_nodes()\n",
    "        metadata['num_edges'] = G.number_of_edges()\n",
    "        \n",
    "        return G, metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        metadata['error'] = str(e)\n",
    "        return None, metadata\n",
    "\n",
    "\n",
    "print(\"‚úÖ CFG extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "extract_all"
   },
   "outputs": [],
   "source": [
    "# Extract CFGs from all binaries\n",
    "print(\"üîç Extracting CFGs from binaries...\\n\")\n",
    "\n",
    "all_graphs = []\n",
    "all_metadata = []\n",
    "\n",
    "# Process benign files\n",
    "benign_dir = 'data/raw/benign'\n",
    "if os.path.exists(benign_dir):\n",
    "    benign_files = [f for f in os.listdir(benign_dir) if os.path.isfile(os.path.join(benign_dir, f))]\n",
    "    print(f\"Processing {len(benign_files)} benign files...\")\n",
    "    \n",
    "    for filename in tqdm(benign_files, desc=\"Benign\"):\n",
    "        file_path = os.path.join(benign_dir, filename)\n",
    "        graph, metadata = extract_cfg_from_binary(file_path, label=0)\n",
    "        \n",
    "        if graph is not None:\n",
    "            all_graphs.append((graph, 0))\n",
    "        all_metadata.append(metadata)\n",
    "\n",
    "# Process malware files\n",
    "malware_dir = 'data/raw/malware'\n",
    "if os.path.exists(malware_dir):\n",
    "    malware_files = [f for f in os.listdir(malware_dir) if os.path.isfile(os.path.join(malware_dir, f))]\n",
    "    print(f\"\\nProcessing {len(malware_files)} malware files...\")\n",
    "    \n",
    "    for filename in tqdm(malware_files, desc=\"Malware\"):\n",
    "        file_path = os.path.join(malware_dir, filename)\n",
    "        graph, metadata = extract_cfg_from_binary(file_path, label=1)\n",
    "        \n",
    "        if graph is not None:\n",
    "            all_graphs.append((graph, 1))\n",
    "        all_metadata.append(metadata)\n",
    "\n",
    "# Statistics\n",
    "successful = sum(1 for m in all_metadata if m['status'] == 'success')\n",
    "failed = len(all_metadata) - successful\n",
    "\n",
    "print(f\"\\n‚úÖ CFG Extraction Complete:\")\n",
    "print(f\"   Successful: {successful}/{len(all_metadata)}\")\n",
    "print(f\"   Failed:     {failed}/{len(all_metadata)}\")\n",
    "print(f\"   Total graphs: {len(all_graphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "features"
   },
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Extract node features from CFGs and convert to PyTorch Geometric format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_functions"
   },
   "outputs": [],
   "source": [
    "def extract_node_features(node_id, graph, node_attrs):\n",
    "    \"\"\"\n",
    "    Extract features for a single node (basic block)\n",
    "    \n",
    "    Returns:\n",
    "        list: Feature vector [10 features]\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Feature 1: Node size (instruction count)\n",
    "    size = 1\n",
    "    if isinstance(node_id, str) and '[' in node_id and ']' in node_id:\n",
    "        try:\n",
    "            size = int(node_id.split('[')[-1].split(']')[0])\n",
    "        except:\n",
    "            size = 1\n",
    "    features.append(float(size))\n",
    "    \n",
    "    # Feature 2: In-degree\n",
    "    in_degree = graph.in_degree(node_id)\n",
    "    features.append(float(in_degree))\n",
    "    \n",
    "    # Feature 3: Out-degree\n",
    "    out_degree = graph.out_degree(node_id)\n",
    "    features.append(float(out_degree))\n",
    "    \n",
    "    # Feature 4: Is entry node\n",
    "    features.append(1.0 if in_degree == 0 else 0.0)\n",
    "    \n",
    "    # Feature 5: Is exit node\n",
    "    features.append(1.0 if out_degree == 0 else 0.0)\n",
    "    \n",
    "    # Feature 6: Is hub node\n",
    "    features.append(1.0 if (in_degree > 2 and out_degree > 2) else 0.0)\n",
    "    \n",
    "    # Feature 7: Degree ratio\n",
    "    degree_ratio = float(out_degree) / (float(in_degree) + 1.0)\n",
    "    features.append(degree_ratio)\n",
    "    \n",
    "    # Feature 8: Is branching node\n",
    "    features.append(1.0 if out_degree > 1 else 0.0)\n",
    "    \n",
    "    # Feature 9: Is merge node\n",
    "    features.append(1.0 if in_degree > 1 else 0.0)\n",
    "    \n",
    "    # Feature 10: Log of size\n",
    "    log_size = math.log(size + 1)\n",
    "    features.append(log_size)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def cfg_to_pyg_data(graph, label):\n",
    "    \"\"\"\n",
    "    Convert NetworkX CFG to PyTorch Geometric Data object\n",
    "    \n",
    "    Args:\n",
    "        graph: NetworkX graph\n",
    "        label: 0 for benign, 1 for malware\n",
    "    \n",
    "    Returns:\n",
    "        Data: PyTorch Geometric Data object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if graph.number_of_nodes() == 0:\n",
    "            return None\n",
    "        \n",
    "        # Create integer node mapping\n",
    "        node_list = list(graph.nodes())\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(node_list)}\n",
    "        \n",
    "        # Relabel nodes\n",
    "        G = nx.relabel_nodes(graph, node_to_idx)\n",
    "        \n",
    "        # Extract node features\n",
    "        node_features = []\n",
    "        for node_id in range(len(node_list)):\n",
    "            original_node = node_list[node_id]\n",
    "            attrs = G.nodes[node_id]\n",
    "            features = extract_node_features(original_node, G, attrs)\n",
    "            node_features.append(features)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        # Create edge index\n",
    "        edge_list = list(G.edges())\n",
    "        if len(edge_list) == 0:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        else:\n",
    "            edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Create label\n",
    "        y = torch.tensor([label], dtype=torch.long)\n",
    "        \n",
    "        # Create PyG Data object\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data.num_nodes = len(node_list)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing graph: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "convert_to_pyg"
   },
   "outputs": [],
   "source": [
    "# Convert all graphs to PyG Data objects\n",
    "print(\"üîß Converting CFGs to PyTorch Geometric format...\\n\")\n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "for graph, label in tqdm(all_graphs, desc=\"Converting\"):\n",
    "    data = cfg_to_pyg_data(graph, label)\n",
    "    if data is not None:\n",
    "        dataset.append(data)\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"\\n‚úÖ Conversion Complete:\")\n",
    "print(f\"   Total samples: {len(dataset)}\")\n",
    "print(f\"   Benign:  {labels.count(0)}\")\n",
    "print(f\"   Malware: {labels.count(1)}\")\n",
    "\n",
    "if len(dataset) > 0:\n",
    "    print(f\"\\nüìä Sample statistics:\")\n",
    "    print(f\"   Features per node: {dataset[0].x.shape[1]}\")\n",
    "    print(f\"   Average nodes per graph: {np.mean([d.num_nodes for d in dataset]):.1f}\")\n",
    "    print(f\"   Min nodes: {min([d.num_nodes for d in dataset])}\")\n",
    "    print(f\"   Max nodes: {max([d.num_nodes for d in dataset])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## 5. Model Definition\n",
    "\n",
    "Define Graph Neural Network architectures for malware classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_class"
   },
   "outputs": [],
   "source": [
    "class MalwareGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Network for Malware Classification\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels=64, num_classes=2,\n",
    "                 dropout=0.5, pooling='mean'):\n",
    "        super(MalwareGCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.pooling = pooling\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Global pooling\n",
    "        if self.pooling == 'mean':\n",
    "            x = global_mean_pool(x, batch)\n",
    "        else:\n",
    "            x = global_max_pool(x, batch)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class MalwareGAT(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Network for Malware Classification\n",
    "    \"\"\"\n",
    "    def __init__(self, num_node_features, hidden_channels=64, num_classes=2,\n",
    "                 heads=4, dropout=0.5, pooling='mean'):\n",
    "        super(MalwareGAT, self).__init__()\n",
    "        \n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1)\n",
    "        self.fc = nn.Linear(hidden_channels, num_classes)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.pooling = pooling\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # First GAT layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Second GAT layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Global pooling\n",
    "        if self.pooling == 'mean':\n",
    "            x = global_mean_pool(x, batch)\n",
    "        else:\n",
    "            x = global_max_pool(x, batch)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def create_model(config):\n",
    "    \"\"\"Factory function to create model\"\"\"\n",
    "    if config['model_type'] == 'gcn':\n",
    "        return MalwareGCN(\n",
    "            num_node_features=config['num_features'],\n",
    "            hidden_channels=config['hidden_channels'],\n",
    "            num_classes=config['num_classes'],\n",
    "            dropout=config['dropout'],\n",
    "            pooling=config['pooling']\n",
    "        )\n",
    "    elif config['model_type'] == 'gat':\n",
    "        return MalwareGAT(\n",
    "            num_node_features=config['num_features'],\n",
    "            hidden_channels=config['hidden_channels'],\n",
    "            num_classes=config['num_classes'],\n",
    "            dropout=config['dropout'],\n",
    "            pooling=config['pooling']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {config['model_type']}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Model architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "split"
   },
   "source": [
    "### Data Splitting\n",
    "\n",
    "Split dataset into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split_data"
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "if len(dataset) > 0:\n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    # First split: train vs (val + test)\n",
    "    train_indices, temp_indices = train_test_split(\n",
    "        indices,\n",
    "        test_size=(CONFIG['val_ratio'] + CONFIG['test_ratio']),\n",
    "        stratify=labels,\n",
    "        random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    # Second split: val vs test\n",
    "    temp_labels = [labels[i] for i in temp_indices]\n",
    "    val_size = CONFIG['val_ratio'] / (CONFIG['val_ratio'] + CONFIG['test_ratio'])\n",
    "    \n",
    "    val_indices, test_indices = train_test_split(\n",
    "        temp_indices,\n",
    "        test_size=(1 - val_size),\n",
    "        stratify=temp_labels,\n",
    "        random_state=CONFIG['seed']\n",
    "    )\n",
    "    \n",
    "    # Create subsets\n",
    "    train_dataset = [dataset[i] for i in train_indices]\n",
    "    val_dataset = [dataset[i] for i in val_indices]\n",
    "    test_dataset = [dataset[i] for i in test_indices]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    \n",
    "    print(f\"‚úÖ Data split complete:\")\n",
    "    print(f\"   Train: {len(train_dataset)} samples\")\n",
    "    print(f\"   Val:   {len(val_dataset)} samples\")\n",
    "    print(f\"   Test:  {len(test_dataset)} samples\")\n",
    "    \n",
    "    # Calculate class weights\n",
    "    if CONFIG['use_class_weights']:\n",
    "        num_benign = labels.count(0)\n",
    "        num_malware = labels.count(1)\n",
    "        total = len(labels)\n",
    "        \n",
    "        weight_benign = total / (2 * num_benign) if num_benign > 0 else 1.0\n",
    "        weight_malware = total / (2 * num_malware) if num_malware > 0 else 1.0\n",
    "        \n",
    "        class_weights = torch.tensor([weight_benign, weight_malware]).to(device)\n",
    "        print(f\"\\n   Class weights: [{weight_benign:.3f}, {weight_malware:.3f}]\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "else:\n",
    "    print(\"‚ùå No data available for splitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 6. Training\n",
    "\n",
    "Train the GNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training_functions"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, class_weights=None):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(data)\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            loss = F.nll_loss(out, data.y, weight=class_weights)\n",
    "        else:\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, class_weights=None):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            loss = F.nll_loss(out, data.y, weight=class_weights)\n",
    "        else:\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "        \n",
    "        pred = out.argmax(dim=1)\n",
    "        probs = torch.exp(out)\n",
    "        \n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(data.y.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        correct += (pred == data.y).sum().item()\n",
    "        total += data.y.size(0)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return (total_loss / len(loader), correct / total, \n",
    "            np.array(all_preds), np.array(all_labels), np.array(all_probs))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if len(dataset) > 0:\n",
    "    print(\"üöÄ Starting training...\\n\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(CONFIG).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"üìä Model: {CONFIG['model_type'].upper()}\")\n",
    "    print(f\"   Parameters: {num_params:,}\\n\")\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, optimizer, device, class_weights\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, _, _, _ = evaluate(\n",
    "            model, val_loader, device, class_weights\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\n‚èπÔ∏è  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 7. Evaluation & Visualization\n",
    "\n",
    "Evaluate the trained model and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_test"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "if len(dataset) > 0:\n",
    "    print(\"üìä Evaluating on test set...\\n\")\n",
    "    \n",
    "    test_loss, test_acc, test_preds, test_labels, test_probs = evaluate(\n",
    "        model, test_loader, device, class_weights\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Results:\")\n",
    "    print(f\"  Loss:     {test_loss:.4f}\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nüìã Classification Report:\\n\")\n",
    "    print(classification_report(\n",
    "        test_labels, test_preds,\n",
    "        target_names=['Benign', 'Malware'],\n",
    "        digits=4\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_training"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if len(dataset) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_confusion"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "if len(dataset) > 0:\n",
    "    cm = confusion_matrix(test_labels, test_preds)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Benign', 'Malware'],\n",
    "                yticklabels=['Benign', 'Malware'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    \n",
    "    # Add percentages\n",
    "    total = np.sum(cm)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            percentage = cm[i, j] / total * 100\n",
    "            plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)',\n",
    "                    ha='center', va='center', fontsize=10, color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print confusion matrix details\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\nüìä Confusion Matrix Details:\")\n",
    "    print(f\"   True Negatives (TN):  {tn} (Correctly identified benign)\")\n",
    "    print(f\"   False Positives (FP): {fp} (Benign misclassified as malware)\")\n",
    "    print(f\"   False Negatives (FN): {fn} (Malware misclassified as benign) ‚ö†Ô∏è\")\n",
    "    print(f\"   True Positives (TP):  {tp} (Correctly identified malware)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_roc"
   },
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "if len(dataset) > 0:\n",
    "    malware_probs = test_probs[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels, malware_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=3,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',\n",
    "             label='Random classifier')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve',\n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\", fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéØ ROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_pr"
   },
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "if len(dataset) > 0:\n",
    "    precision, recall, _ = precision_recall_curve(test_labels, malware_probs)\n",
    "    avg_precision = average_precision_score(test_labels, malware_probs)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=3,\n",
    "             label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=12)\n",
    "    plt.ylabel('Precision', fontsize=12)\n",
    "    plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "    plt.legend(loc=\"lower left\", fontsize=11)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüéØ Average Precision Score: {avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üìä Summary\n",
    "\n",
    "Generate a comprehensive summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary_stats"
   },
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if len(dataset) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä FINAL SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nDataset:\")\n",
    "    print(f\"  Total samples:    {len(dataset)}\")\n",
    "    print(f\"  Benign samples:   {labels.count(0)}\")\n",
    "    print(f\"  Malware samples:  {labels.count(1)}\")\n",
    "    print(f\"\\nModel:\")\n",
    "    print(f\"  Architecture:     {CONFIG['model_type'].upper()}\")\n",
    "    print(f\"  Parameters:       {num_params:,}\")\n",
    "    print(f\"  Hidden channels:  {CONFIG['hidden_channels']}\")\n",
    "    print(f\"\\nTraining:\")\n",
    "    print(f\"  Epochs trained:   {len(history['train_loss'])}\")\n",
    "    print(f\"  Best val acc:     {best_val_acc:.4f}\")\n",
    "    print(f\"\\nTest Performance:\")\n",
    "    print(f\"  Accuracy:         {test_acc:.4f}\")\n",
    "    print(f\"  ROC AUC:          {roc_auc:.4f}\")\n",
    "    print(f\"  Avg Precision:    {avg_precision:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Negatives:   {tn}\")\n",
    "    print(f\"  False Positives:  {fp}\")\n",
    "    print(f\"  False Negatives:  {fn}\")\n",
    "    print(f\"  True Positives:   {tp}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ Analysis Complete!\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## üíæ Save Model & Results\n",
    "\n",
    "Save the trained model and results for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save model and results\n",
    "if len(dataset) > 0:\n",
    "    # Save model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': CONFIG,\n",
    "        'test_accuracy': test_acc,\n",
    "        'roc_auc': roc_auc\n",
    "    }, 'malware_gnn_model.pt')\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'config': CONFIG,\n",
    "        'history': history,\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_loss': float(test_loss),\n",
    "        'roc_auc': float(roc_auc),\n",
    "        'avg_precision': float(avg_precision),\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "    \n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(\"‚úÖ Model and results saved!\")\n",
    "    print(\"   - malware_gnn_model.pt\")\n",
    "    print(\"   - results.json\")\n",
    "    \n",
    "    # Download files\n",
    "    print(\"\\nüì• Download files:\")\n",
    "    files.download('malware_gnn_model.pt')\n",
    "    files.download('results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "To improve your model:\n",
    "\n",
    "1. **Collect more data** - Aim for 1000+ samples per class\n",
    "2. **Try different architectures** - Change `CONFIG['model_type']` to 'gat'\n",
    "3. **Add more features** - Modify `extract_node_features()` function\n",
    "4. **Tune hyperparameters** - Adjust learning rate, hidden channels, dropout\n",
    "5. **Multi-class classification** - Classify by malware family\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Safety Reminder\n",
    "\n",
    "**Always work with malware in isolated environments!**\n",
    "- Use VMs with no network access\n",
    "- Never execute malware samples\n",
    "- Take regular snapshots\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Malware Hunting! üõ°Ô∏èüîç**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
